{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d302c9da-fb72-4f48-a62f-bd3bd01cb572",
   "metadata": {},
   "source": [
    "### Intasll packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56a385-c062-49f6-a6bb-e4aa35fcbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google.cloud pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cbbb2-b983-4f8b-b1e2-bfa8d604350f",
   "metadata": {},
   "source": [
    "### Set env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910fd76-73fa-4d6d-b49c-d0d1783fc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env IAP_CLIENT_ID=\"389410459067-mltiuc7631od8mhp9aokhb03qdlj81qp.apps.googleusercontent.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932641a-b368-42ec-b075-eefb5f959252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "mlflow_token=subprocess.getoutput(\"\"\"curl -s -X POST -H \"content-type: application/json\" -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -d \"{\\\"audience\\\": \\\"${IAP_CLIENT_ID}\\\", \\\"includeEmail\\\": true }\" \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/$(gcloud auth list --filter=status:ACTIVE --format='value(account)'):generateIdToken\"  | jq -r '.token'\"\"\")\n",
    "os.environ['MLFLOW_TRACKING_TOKEN'] = mlflow_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd71bd-f603-4a8b-8e87-28b7f3b86341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MLFLOW_TRACKING_URI=https://mlflow-dot-tbd-2023l-mlops.ew.r.appspot.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33653890-70e6-44a7-83a0-018b094e0416",
   "metadata": {},
   "source": [
    "### Test connectivity with MLflow tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c5889-b773-4339-bb96-47526849fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mlflow experiments search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d325e150-9419-457b-b2d4-d3c3cd3cb738",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81051147-75d1-4680-9c78-128ad77d95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil mb -l europe-west1 gs://tbd-2023l-2001-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b816d-bffe-48fb-a40f-a5bc7e4f5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -L https://github.com/datascienceverse/stack-overflow-dataset-2022/raw/master/survey_results_public.csv | gsutil cp - gs://tbd-2023l-2001-data/survey_results_public.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091d034-2fec-46e8-81c1-d16d346b74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil du -h gs://tbd-2023l-2001-data/survey_results_public.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa24506-3817-4c40-a0ac-3b5857aebe06",
   "metadata": {},
   "source": [
    "### GCS connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd8240-79c7-4319-833e-f9737b299a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.9/gcs-connector-hadoop3-2.2.9-shaded.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d8765-fa84-4e98-93e6-9c7dbf79c2d0",
   "metadata": {},
   "source": [
    "### Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2d07e-3aa6-4de3-aef0-1275da634491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".master('yarn') \\\n",
    ".config('spark.executor.instance',2) \\\n",
    ".config('spark.jars','gcs-connector-hadoop3-2.2.9-shaded.jar') \\\n",
    ".config('spark.jars.packages','org.mlflow:mlflow-spark:1.11.0') \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3e80f-de20-4608-a4c6-0087cd384c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200ffc5-1b16-42ee-ad6b-6daae6ce5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"tbd\"\n",
    "gs_path = \"gs://tbd-2023l-2001-data/survey_results_public.csv\"\n",
    "spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
    "spark.sql(f'CREATE DATABASE {db_name}')\n",
    "spark.sql(f'USE {db_name}')\n",
    "table_name = \"survey_2022\" \n",
    "\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n",
    "\n",
    "spark_df= spark.sql(f'SELECT *, CAST((ConvertedCompYearly > 60000) AS STRING) AS compAboveAvg \\\n",
    "                    FROM {table_name} WHERE ConvertedCompYearly IS NOT NULL ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70a4d6-5073-414c-8606-738482669775",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46b8a2-66ea-4a9e-9d19-b73e728692ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "y = 'compAboveAvg' \n",
    "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'YearsCode']\n",
    "\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
    "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\")]\n",
    "\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]\n",
    "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features') \n",
    "\n",
    "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
    "\n",
    "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
    "                          onehotencoder_stages + \\\n",
    "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
    "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234) # Podzial na zbior treningowy/testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2ce47-9383-4d74-a763-368f0b92d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow   ## zaimportowanie modulu \n",
    "import mlflow.spark\n",
    "\n",
    "## tworzymy nowy eksperyment - powinien się pojawić w UI ML Flow. \n",
    "## Jesli nie tworzymy nowego eksperymentu nowe przebiegi beda sie zapisywac pod domyslnym (default)\n",
    "ename = f\"tbd-2023l-2001\"\n",
    "artifacts_location= \"artifacts\"\n",
    "mlflow.set_experiment(experiment_name=ename)\n",
    "experiment = mlflow.get_experiment_by_name(ename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d43357-47be-45d0-b083-2c424cf82931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_f = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53413c-3af6-4646-8c2f-b7f68846db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id):\n",
    "    dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
    "    mlflow.set_tag(\"classifier\", \"decision_tree\")  ## ustawienie tagow\n",
    "    mlflow.log_param(\"depth\", dt.getMaxDepth())    ## zapisanie metadanych - hiperparametrow\n",
    "\n",
    "    dt_model = Pipeline(stages=[dt]).fit(training)\n",
    "    pred_dt = dt_model.transform(test)\n",
    "    label_and_pred = pred_dt.select('label', 'prediction')\n",
    "    res = dt_model.transform(test)\n",
    "\n",
    "    test_metric_acc = evaluator_acc.evaluate(res)\n",
    "    test_metric_recall = evaluator_recall.evaluate(res)\n",
    "    test_metric_prec = evaluator_prec.evaluate(res)\n",
    "    test_metric_f = evaluator_f.evaluate(res)\n",
    "\n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f)\n",
    "    mlflow.spark.log_model(dt_model, artifact_path=artifacts_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54490f91-2a9c-4715-9929-dedb8885c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc838e7-3532-4118-9ba6-eec76f4dc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "gbt_model = gbt.fit(training)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"gbt_model\"):\n",
    "  \n",
    "    mlflow.log_param(\"depth\", gbt.getMaxDepth())\n",
    "\n",
    "    res = gbt_model.transform(test)\n",
    "    \n",
    "    test_metric_acc = evaluator_acc.evaluate(res)\n",
    "    test_metric_recall = evaluator_recall.evaluate(res)\n",
    "    test_metric_prec = evaluator_prec.evaluate(res)\n",
    "    test_metric_f = evaluator_f.evaluate(res)\n",
    "\n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=gbt_model, artifact_path='gbt_classifier') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5be9c-2aff-445e-a4db-a3ac7911960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training, raw_test = spark_df.randomSplit([0.8, 0.2], seed=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6004c-9e27-4f76-922a-3a1767959d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "logged_model = 'runs:/e08c7cc74cad4df6a1adb0939dca91f9/gbt_classifier'\n",
    "pyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed332657-1ef6-44e6-9456-02c61ce48965",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = raw_test.limit(10)\\\n",
    "    .withColumn(\"prediction\", pyfunc_udf(struct('OpSys', 'EdLevel', 'MainBranch' , 'Country', 'YearsCode'))) \\\n",
    "    .select ('OpSys', 'EdLevel', 'MainBranch' , 'Country', 'YearsCode', 'prediction')\n",
    "predicted_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1afc0-dd8f-4a41-ae9a-e8fa3cdebc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b387fba-4fa9-41e2-ae94-2a6cb0c4d78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
