{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD Phase 2: Performance & Computing Models\n",
    "\n",
    "## Introduction\n",
    "In this lab, you will compare the performance and computing models of four popular data processing engines: **Polars, Pandas, DuckDB, and PySpark**.\n",
    "\n",
    "You will explore:\n",
    "- **Performance**: Single-node processing speed, parallel execution, and memory usage.\n",
    "- **Scalability**: How performance changes with the number of cores (single-node) and executors (cluster).\n",
    "- **Computing Models**: Out-of-core vs. In-memory processing, and Eager vs. Lazy execution.\n",
    "\n",
    "### Engine Capabilities\n",
    "The following table summarizes the key capabilities of the engines we will be testing. Use this as a reference.\n",
    "\n",
    "| Engine | Query Optimizer | Distributed | Arrow-backed | Out-of-Core | Parallel | APIs | GPU Support |\n",
    "|---|---|---|---|---|--|---|---|\n",
    "| **Pandas** | ❌ | ❌ | optional ≥ 2.0 | ❌ | ❌ | DataFrame | ❌ |\n",
    "| **Polars** | ✅ | ❌ | ✅ | ✅ | ✅ | DataFrame | ✅ (opt) |\n",
    "| **PySpark** | ✅ | ✅ | Pandas UDF/IO | ✅ | ✅ | SQL, DataFrame | ❌ (no GPU) |\n",
    "| **DuckDB** | ✅ | ❌ | ✅ | ✅ | ❌ | SQL, Relational API | ❌ |\n",
    "\n",
    "## Prerequisites\n",
    "Ensure you have the necessary libraries installed."
   ],
   "id": "711646c422baab35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars pandas duckdb pyspark faker deltalake memory_profiler pyarrow"
   ],
   "id": "f03c89d6004dfa00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Initialize Spark (Single Node)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataLab2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "7d4c5b1b58dc798a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Generation\n",
    "\n",
    "We will generate a synthetic dataset simulating social media posts with a rich schema.\n",
    "\n",
    "**Schema**:\n",
    "- `post_id` (String): Unique identifier.\n",
    "- `user_id` (Integer): User identifier.\n",
    "- `timestamp` (DateTime): Time of post.\n",
    "- `content` (String): Text content.\n",
    "- `likes` (Integer): Number of likes.\n",
    "- `views` (Integer): Number of views.\n",
    "- `category` (String): Post category.\n",
    "- `tags` (List[String]): Hashtags.\n",
    "- `location` (String): User location.\n",
    "- `device` (String): Device used (Mobile, Web, etc.).\n",
    "- `latency` (Float): Network latency.\n",
    "- `error_rate` (Float): Error rate during upload."
   ],
   "id": "c1fdbc3b29449e48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_records=1_000_000, output_path=\"social_media_data.parquet\"):\n",
    "    fake = Faker()\n",
    "    \n",
    "    print(f\"Generating {num_records} records...\")\n",
    "    \n",
    "    # Generate data using numpy for speed where possible\n",
    "    data = {\n",
    "        \"post_id\": [fake.uuid4() for _ in range(num_records)],\n",
    "        \"user_id\": np.random.randint(1, 100_000, num_records),\n",
    "        \"timestamp\": pd.date_range(start=\"2023-01-01\", periods=num_records, freq=\"s\").to_numpy().astype(\"datetime64[us]\"),\n",
    "        \"likes\": np.random.randint(0, 10_000, num_records),\n",
    "        \"views\": np.random.randint(0, 1_000_000, num_records),\n",
    "        \"category\": np.random.choice([\"Tech\", \"Health\", \"Travel\", \"Food\", \"Fashion\", \"Politics\", \"Sports\"], num_records),\n",
    "        \"tags\": [np.random.choice([\"#viral\", \"#new\", \"#trending\", \"#hot\", \"#update\"], size=np.random.randint(1, 4)).tolist() for _ in range(num_records)],\n",
    "        \"location\": np.random.choice([\"USA\", \"UK\", \"DE\", \"PL\", \"FR\", \"JP\", \"BR\"], num_records),\n",
    "        \"device\": np.random.choice([\"Mobile\", \"Desktop\", \"Tablet\"], num_records),\n",
    "        \"latency\": np.random.uniform(10.0, 500.0, num_records),\n",
    "        \"error_rate\": np.random.beta(1, 10, num_records),\n",
    "        \"content\": [fake.sentence() for _ in range(min(num_records, 1000))] * (num_records // 1000 + 1)\n",
    "    }\n",
    "    \n",
    "    # Trim to exact size\n",
    "    data[\"content\"] = data[\"content\"][:num_records]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"Writing to Parquet...\")\n",
    "    df.to_parquet(output_path, engine=\"pyarrow\")\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# Generate 5 million records\n",
    "generate_data(num_records=5_000_000)"
   ],
   "id": "8f6706879ef7347"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Measuring Performance\n",
    "\n",
    "### 2.1 Execution Time\n",
    "Use `%time` or `%timeit` to measure execution time."
   ],
   "id": "ecc9302afb73233"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Measuring time for all engines\n",
    "print(\"--- Performance Benchmark Example ---\")\n",
    "\n",
    "# Pandas\n",
    "print(\"Pandas Load Time:\")\n",
    "%time df_pd = pd.read_parquet(\"social_media_data.parquet\")\n",
    "\n",
    "# Polars\n",
    "print(\"\\nPolars Load Time:\")\n",
    "%time df_pl = pl.read_parquet(\"social_media_data.parquet\")\n",
    "\n",
    "# DuckDB\n",
    "print(\"\\nDuckDB Query Time:\")\n",
    "%time duckdb.sql(\"SELECT count(*) FROM 'social_media_data.parquet'\").show()\n",
    "\n",
    "# PySpark\n",
    "print(\"\\nSpark Load Time:\")\n",
    "%time df_spark = spark.read.parquet(\"social_media_data.parquet\"); df_spark.count()"
   ],
   "id": "d1853bedbac6744f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Student Tasks\n",
    "\n",
    "### Task 1: Performance & Scalability (Single Node)\n",
    "\n",
    "**Goal**: Benchmark the engines and test how they scale with available CPU cores.\n",
    "\n",
    "**Instructions**:\n",
    "1.  **Define Queries**: Create 3 distinct queries of your own choice. They should cover:\n",
    "    -   **Query A**: A simple aggregation (e.g., grouping by a categorical column and calculating means).\n",
    "    -   **Query B**: A window function or more complex transformation.\n",
    "    -   **Query C**: A join (e.g., self-join or join with a smaller generated table) with filtering.\n",
    "2.  **Benchmark**: Implement these queries in **Pandas, Polars, DuckDB, and PySpark**.\n",
    "    -   Measure **Execution Time** using `%time` or `time.time()`.\n",
    "    -   Measure **Peak Memory** usage using `memory_profiler` (e.g., `memory_usage()`).\n",
    "3.  **Scalability Test**: \n",
    "    -   Select **all engines** that support parallel execution on a single node (e.g., Polars, DuckDB).\n",
    "    -   Run **all 3 queries** with different numbers of threads/cores (e.g., 1, 2, 4, 8).\n",
    "    -   Plot the speedup for each query and engine.\n",
    "\n",
    "**Tip**: \n",
    "-   Polars: [polars.thread_pool_size](https://docs.pola.rs/api/python/stable/reference/api/polars.thread_pool_size.html) Please also note that *Thread configuration in Polars requires process restart*\n",
    "-   DuckDB: `PRAGMA threads=n`\n",
    "-   Spark: `master=\"local[n]\"`"
   ],
   "id": "6e72523f0151fa3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here for Task 1"
   ],
   "id": "15c42c1fd440c982"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Spark on Cluster\n",
    "\n",
    "**Goal**: Compare Single Node performance vs. Spark on a Cluster.\n",
    "\n",
    "**Instructions**:\n",
    "1.  **Infrastructure**: Use the infrastructure from **Phase 1** (Google Dataproc). You may need to modify your Terraform code to adjust the cluster configuration (e.g., number of worker nodes).\n",
    "2.  **Environment**: The easiest way to run this is via **Google Workbench** connected to your Dataproc cluster.\n",
    "3.  **Upload Data**: Upload the generated `social_media_data.parquet` to HDFS or GCS.\n",
    "    -   **Tip**: For better performance, consider **partitioning** the data (e.g., by `category` or `date`) when saving it to the distributed storage. This allows Spark to optimize reads.\n",
    "4.  **Run Queries**: Run your PySpark queries from Task 1 on the cluster.\n",
    "5.  **Scalability Test**: \n",
    "    -   Run the queries with different numbers of **worker nodes** (e.g., 2, 3, 4).\n",
    "    -   You can achieve this by resizing the cluster (manually or via Terraform) or by configuring the number of executors in Spark.\n",
    "6.  **Analyze**:\n",
    "    -   How does the cluster performance compare to your local machine?\n",
    "    -   Did adding more nodes/executors linearly improve performance?\n",
    "    -   **Tip**: If Spark is slower than single-node engines, consider **increasing the dataset size** (e.g., generate 10M+ records or duplicate the data). Spark's overhead is significant for small data, and its true power appears when data exceeds single-node memory."
   ],
   "id": "87f48ffba51c7c3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here for Task 2"
   ],
   "id": "40672b4a6e5fa43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Execution Modes & Analysis\n",
    "\n",
    "**Goal**: Deep dive into execution models and limitations.\n",
    "\n",
    "**Instructions**:\n",
    "1.  **Lazy vs. Eager vs. Streaming**:\n",
    "    -   Use **Polars**. Compare the **Execution Time** and **Peak Memory** of:\n",
    "        -   Eager execution (`read_parquet` -> filter).\n",
    "        -   Lazy execution (`scan_parquet` -> filter -> `collect()`).\n",
    "        -   Streaming execution (`scan_parquet` -> filter -> `collect(streaming=True)`).\n",
    "2.  **Polars Limitations**:\n",
    "    -   Identify a scenario where Polars might struggle compared to Spark (e.g., memory limits).\n",
    "3.  **Decision Boundary**:\n",
    "    -   Based on your findings, when would you recommend switching from a single-node tool (Polars/DuckDB) to a distributed engine (Spark)?"
   ],
   "id": "ceb0f29d88d7c0f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here for Task 3"
   ],
   "id": "d11652ae7fcf51be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
