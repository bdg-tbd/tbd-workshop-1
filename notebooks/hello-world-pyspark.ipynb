{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c7bb92a-c8d7-4155-ba87-3e908a2f0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 22:16:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TBD-workshop-1\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579ecc90-1430-4b00-9604-4790994bded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca4ef063-fff6-403c-a0b4-c817c96e3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 22:18:58 INFO SparkContext: Starting job: count at /tmp/ipykernel_83/2992961663.py:1\n",
      "23/05/10 22:18:58 INFO DAGScheduler: Got job 0 (count at /tmp/ipykernel_83/2992961663.py:1) with 2 output partitions\n",
      "23/05/10 22:18:58 INFO DAGScheduler: Final stage: ResultStage 0 (count at /tmp/ipykernel_83/2992961663.py:1)\n",
      "23/05/10 22:18:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/05/10 22:18:58 INFO DAGScheduler: Missing parents: List()\n",
      "23/05/10 22:18:58 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at count at /tmp/ipykernel_83/2992961663.py:1), which has no missing parents\n",
      "23/05/10 22:18:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.1 KiB, free 434.4 MiB)\n",
      "23/05/10 22:18:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.4 MiB)\n",
      "23/05/10 22:18:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on tbd-2023l-2001-notebook:16385 (size: 4.4 KiB, free: 434.4 MiB)\n",
      "23/05/10 22:18:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "23/05/10 22:18:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at count at /tmp/ipykernel_83/2992961663.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/05/10 22:18:58 INFO YarnScheduler: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "23/05/10 22:18:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (tbd-cluster-w-0.europe-west1-b.c.tbd-2023l-2001.internal, executor 1, partition 0, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
      "23/05/10 22:18:58 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (tbd-cluster-w-1.europe-west1-b.c.tbd-2023l-2001.internal, executor 2, partition 1, PROCESS_LOCAL, 4498 bytes) taskResourceAssignments Map()\n",
      "23/05/10 22:18:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on tbd-cluster-w-1.europe-west1-b.c.tbd-2023l-2001.internal:37351 (size: 4.4 KiB, free: 434.4 MiB)\n",
      "23/05/10 22:18:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on tbd-cluster-w-0.europe-west1-b.c.tbd-2023l-2001.internal:38661 (size: 4.4 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 22:19:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1616 ms on tbd-cluster-w-1.europe-west1-b.c.tbd-2023l-2001.internal (executor 2) (1/2)\n",
      "23/05/10 22:19:00 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 22:19:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1851 ms on tbd-cluster-w-0.europe-west1-b.c.tbd-2023l-2001.internal (executor 1) (2/2)\n",
      "23/05/10 22:19:00 INFO DAGScheduler: ResultStage 0 (count at /tmp/ipykernel_83/2992961663.py:1) finished in 1.882 s\n",
      "23/05/10 22:19:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/05/10 22:19:00 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/05/10 22:19:00 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "23/05/10 22:19:00 INFO DAGScheduler: Job 0 finished: count at /tmp/ipykernel_83/2992961663.py:1, took 1.895998 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 22:23:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on tbd-cluster-w-0.europe-west1-b.c.tbd-2023l-2001.internal:38661 in memory (size: 4.4 KiB, free: 434.4 MiB)\n",
      "23/05/10 22:23:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on tbd-cluster-w-1.europe-west1-b.c.tbd-2023l-2001.internal:37351 in memory (size: 4.4 KiB, free: 434.4 MiB)\n",
      "23/05/10 22:23:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on tbd-2023l-2001-notebook:16385 in memory (size: 4.4 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.parallelize((1,1,1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60cf3a3-71f0-4280-867e-839ed1b99fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be7088e2-bd21-4f7d-aa93-e0576f6513c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://tbd-2023l-2001-notebook:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TBD-workshop-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f04cefece20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b24b4e-709d-4b1c-ab8e-ea40ad0cb52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
